1. 基本原理[[python 决策树算法原理及基于numpy的代码实现#1 基本原理]]
2. 决策树的构造方法
	1. 信息的不纯度
		1. 信息熵的计算[[python 决策树算法原理及基于numpy的代码实现#2 1 2 信息增益（INFORMATION GAIN - ID3）]]
		2. 信息增益（Information Gain - ID3）[[python 决策树算法原理及基于numpy的代码实现#2 1 2 信息增益（INFORMATION GAIN - ID3）]]
		3. 信息增益率（Information Gain Ratio - C4.5）[[python 决策树算法原理及基于numpy的代码实现#2 1 3 信息增益率（Information Gain Ratio - C4 5）]]
		4. 基尼系数（Gini Index - CART）[[python 决策树算法原理及基于numpy的代码实现#2 1 4 基尼系数（GINI INDEX - CART）]]
	2. 特征的最佳切分点
		1. 离散型变量的最佳切分点划分[[python 决策树算法原理及基于numpy的代码实现#2 2 1 离散型变量的最佳切分点划分]]
		2. 连续型变量的最佳切分点划分[[python 决策树算法原理及基于numpy的代码实现#2 2 2 连续型变量的最佳切分点划分]]
	3. 特征在决策树中决策的先后顺序[[python 决策树算法原理及基于numpy的代码实现#2 3 特征在决策树中决策的先后顺序]]
	4. ID3, C4.5, CART对比说明[[python 决策树算法原理及基于numpy的代码实现#2 4 ID3 C4 5 CART对比说明]]


## 1. 基本原理
决策树本身的原理其实很好理解，就是不断做双向选择题，使用多个特征依次进行样本归类，并根据选择输出某一分类。  
这里举一个简单的例子

|序号	|身高（cm）|	头发长度|	声音音调|	性别|
|:---|:---|:---|:---|:---|:---|
|1	|160	|36	|高	|女|
|2	|165	|15	|高	|女|
|3	|163	|28	|高	|女|
|4	|170	|22	|低	|女|
|5	|177	|3	|低	|男|
|6	|183	|5	|低	|男|
|7	|170	|2	|低	|男|
|8	|176	|10	|低	|男
|9	|184	|13	|低	|男|
例如需要训练一个鉴别男女性别的模型，根据身高、头发长度、声音音调的高低，3个变量来构造一颗决策树。这3个变量，可对应生成出3道类似如下的双向选择题：

- 身高是否高于\*\*\*cm
- 头发是否长于\*\*\*cm  
- 声音是否低沉

例如构造如下的一颗决策树：
![](20200322153531390.png)
## 2. 决策树的构造方法
决策树如何构造才能最高效的对样本进行正确分类，首先需要先弄清楚以下几个点：

1.  信息的不纯度
2.  特征分裂的最佳切分点
3.  特征在决策树中分裂的先后顺序

### 2.1 信息的不纯度
不纯度是在决策树中衡量特征分裂优异性的最主要的指标，用于衡量样本在根据某一特征的分类标准分裂后，样本是否被正确分类的准确程度。
主要有3种计算方式，分别对应了3类决策树：

|决策属类别|不纯度计算方式|
|:---|:---|
|ID3|信息增益（Information Gain）|
|C4.5|信息增益率（Information Gain Ratio）|
|CART|基尼系数（Gini index）|

#### 2.1.1 信息熵的计算

信息熵的计算公式：

$$H(D)=-\sum_{i=1}^{n}p_ilog_2p_i$$

n 为样本 $D$ 中的类别数，例如在该例子中是判断性别的男女，所以n=2 $p_i$ 为该类别所占样本总数的比例，例如本例的样本有9条，女生有4个，$p_{女生}=\frac {4}{9}p$

所以在还未分类时的总样本的信息熵为：
$$H(D)=-\frac{4}{9}log_2\frac{4}{9}-\frac{5}{9}log_2\frac{5}{9}=0.9911$$

#### 2.1.2 信息增益（INFORMATION GAIN - ID3）

信息增益计算公式：

$$Information Gain=g(D,A)=H(D)-H(D|A)=H(D)-\sum_{i=1}^{n}H(D|A_i)$$

$H(D|A)$ 表示在分类后所有分类样本的信息熵之和，$H(D|A_i)$ 表示每一种分类选择的信息增益
在这个例子中，如果我们按照声音的高低来分类，则：

音调高的样本数：3

-   其中女生3个，男生0个

音调低的样本数：6

-   其中女生1个，男生5个

那么该分裂方法的信息增益为:
$$ g(D,声音音调)=H(D)-H(D|声音音调)=H(D)-[H(D|音调高)+H(D|音调低)] $$$$=0.991-[(-\frac33log_2\frac33)+(-\frac16log_2\frac16-\frac56log_2\frac56)]$$$$=0.991-[0+0.6500]=0.3411 $$
#### 2.1.3 信息增益率（Information Gain Ratio - C4.5）

$$IGR=\frac{g(D,A)}{H(A)}$$

$H(A)$ 为A 分类方法下的信息熵
信息增益率越大，表示该分类方法越能区分开样本，即不纯度越小。
还是这个例子，这次用 "身高是否大于175cm"作为分类条件，则：

身高小于等于175cm的有5个

-   其中4个女生，1个男生

身高大于175cm的有4个

-   其中0个女生，4个男生

首先计算信息增益：
$$g(D,身高)=H(D)−[H(D∣身高≤175cm)+H(D∣身高≥175)]$$
$$=0.9911-[(-\frac45log_2\frac45+(-\frac15log_2\frac15))+(-\frac44log_2\frac44)]$$
$$=0.9911-[0.7219+0]=0.2692$$
```ad-blue
- 顺带一提，这里我们相当于已经计算出了"声音音调的高低"和"身高是否大于175cm"两类属性的信息增益，分别为0.3411和0.2692，**信息增益越大，说明信息贡献度越大**，因此可以说，在本例中"声音音调的高低"比"身高是否大于175cm"**区分能力更强**。
```
接下来计算$H(A)$
因为"身高小于等于175cm"的有5个，"身高大于175cm"的有4个，所以
$$H(A)=-\frac59log_2\frac59-\frac49log_2\frac49=0.9911$$
因此
$$ IGR(D,身高是否大于175cm)=\frac{g(D,身高)}{H(A)}=\frac{0.2692}{0.9911}=0.2716$$
#### 2.1.4 基尼系数（GINI INDEX - CART）

$$Gini(p)=\sum_{i-1}^{n}p_i(1-p_i)=1-\sum_{i=1}^{n}p_i^2$$
基尼系数值越小，表示该分类方法贡献的信息度越高，即不纯度越小。
这次用头发长度举例，"头发长度是否大于20cm"，则：  
"头发长大于20cm"的有3个
-   其中3个女生，0个男生

"头发长小于等于20cm"的有6个
-   其中1个女生，5个男生

则：
$$gini(头发长度是否大于20cm)=1-[\frac39\cdot gini(头发长度>20cm)+\frac69 \cdot gini(头发长度≤20cm)]$$
其中：$$gini(头发长度>20cm)=1-(\frac33)^2=0$$
$$gini(头发长度≤20cm)=1-[(\frac16)^2+(\frac56)^2]=\frac5{18}$$
因此：
$$gini(头发长度是否大于20cm)=1-[(\frac39 \times 0)+(\frac69 \times \frac 5{18})]=0.8148$$
```ad-blue
搞清了不纯度的计算后，下面的两个问题就可以比较轻松的解决了，也就是构造一棵决策树最主要的两大问题：
- 特征的最佳切分点
- 特征在决策树中决策的先后顺序
```
### 2.2 特征的最佳切分点
例如在本例中，对于特征 身高是否高于\*\*\*cm，多少的身高阈值才是最好的分裂点呢？怎样计算最佳分裂点呢？

答案就是依据上一节所讲的**不纯度**

首先对于特征的类型分为两种情况：

-   离散型变量
-   连续型变量

#### 2.2.1 离散型变量的最佳切分点划分
在决策树中，离散变量的分叉方法有两类，一类是多叉树，一类是二叉树。

一般来说，CART树为二叉树，C4.5和ID3则可以为多叉树。

**离散变量的二叉树：**
将多属性的变量再拆分为多个"是否"类型的划分问题。例如：
将"声音音调"分为3类属性值：高、中、低
那么在决策树中，需将该特征划分为 $(类别数−1)$ 个分类属性：
- 声音是否是高音调
- 声音是否是中音调

![](20200323133118113.png)
**离散变量的多叉树：**
![](20200323133441236.png)
```ad-blue
不过，在现实世界的算法实现上，由于需要更多考虑算法的运算性能，大部分的库包都没有直接支持离散变量的训练，例如python的sklearn包，当中所有树类模型均为二叉树，且样本输入均限定为必须是数值型，也就是说，若样本中含离散变量，需预先将其利用one-hot编码或者binary编码进行编码变换。
但是这样的话就会很容易面临输入的数据过于稀疏，很多离散属性的样本数量可能本来就很少，导致分裂的信息增益过小，以至于很多样本的分类不准确。目前LightGBM是直接支持离散变量的
```
#### 2.2.2 连续型变量的最佳切分点划分
1. 首先将样本的某一连续变量的值**去重后**按照升序进行排列，记为 $A=\{a_1,a_2,\dots, a_n\}$
2. 计算两两相邻的平均值$\{\frac{a_1+a_2}2,\frac{a_2+a_3}2,\dots,\frac{a_{n-1}+a_n}2\}$，记为$B=\{b_1,b_2,\dots,b_{n-1}\}$
3. 遍历$B$，将$B$的每一个点都作为该连续变量的切分点，并计算其分裂的不纯度，获得长度为$n − 1$ 的不纯度集合，记为$C=\{c_1,c_2,\dots,c_{n-1}\}$
4. $C$中最大的不纯度，其切分点即为最佳切分点。

例如身高这个变量：
1. 按照升序排序：160, 163, 165, 170, 176, 177, 183, 184
2. 求相邻两个的平均值：161.5, 164, 167.5, 173, 176.5, 180, 183.5
3. 以161.5作为分裂点，计算其不纯度，这里以计算信息增益为例，不知道怎么算的回看2.1.2…
4. 分别计算出每个点的不纯度，得到不纯度最大的那个点，即是该变量的最佳分裂点。

### 2.3 特征在决策树中决策的先后顺序
决策的先后顺序，即为根据不同变量进行分裂的顺序。在找出每个变量的最佳分裂点后，可以计算出以该点分裂所能获得的信息度（信息增益/信息增益率/基尼系数…等），以最大信息度的变量放在最前面进行分裂，最小的放在最后面分裂。这样就确定了在对样本进行区分的时候，越早分裂的样本能以最佳的区分方法进行划分。
### 2.4 ID3, C4.5, CART对比说明
|类型|特点|劣势|
|:---|:---|:---|
|ID3|多叉树；特征只用一次；健壮性较好，能训练属性值有缺失的情况|1. 当特征取值类型较多时，信息增益会越大，容易造成过拟合；2. 只能用于分类；3. 只能处理离散变量；4. 对缺失值敏感|
|C4.5|多叉树；特征只用一次；使用信息增益比对特征值多的特征进行惩罚，减少过拟合；可以处理连续变量；可以处理缺失值|处理连续值只是将其离散化，形成多个离散值|
|CART|二叉树；特征使用多次；可以用于回归任务||
