#Tensorflow
取第二层的第 k 个计算单元，可以得到示意图如下：

[![../../_images/neuron.png](https://tf.wiki/_images/neuron.png)](https://tf.wiki/_images/neuron.png)

该计算单元 $Q_k$ 有 100 个权值参数 $w_{0k},w_{1k}...,w_{99k}$ 和 1 个偏置参数 $b_k$。将第 1 层中所有的 100 个计算单元 $P_0,P_1,...,p_99$ 的值作为输入，分别按权值 $w_{ik}$ 加和（即$\sum_{i=0}^{99}w_{ik}P_i$)，并加上偏置值 $b_k$ ，然后送入激活函数 $f$ 进行计算，即得到输出结果。
第二层的每一个计算单元（人工神经元）有 100 个权值参数和 1 个偏置参数，而第二层计算单元的数目是 10 个，因此这一个全连接层的总参数量为 100\*10 个权值参数和 10 个偏置参数。事实上，这正是该全连接层中的两个变量 `kernel` 和 `bias` 的形状。仔细研究一下，你会发现，这里基于神经元建模的介绍与上文基于矩阵计算的介绍是等价的。