#Tensorflow
全连接层（Fully-connected Layer，`tf.keras.layers.Dense` ）是 Keras 中最基础和常用的层之一，对输入矩阵 $A$ 进行$f(AW+b)$ 的线性变换 + 激活函数操作。如果不指定激活函数，即是纯粹的线性变换 $AW+b$。具体而言，给定输入张量 `input = [batch_size, input_dim]` ，该层对输入张量首先进行 `tf.matmul(input, kernel) + bias`[^1] 的线性变换（ `kernel` 和 `bias` 是层中可训练的变量），然后对线性变换后张量的每个元素通过激活函数 `activation` ，从而输出形状为 `[batch_size, units]` 的二维张量。

[![../../_images/dense.png](https://tf.wiki/_images/dense.png)](https://tf.wiki/_images/dense.png)

其包含的主要参数如下：

-   `units` ：输出张量的维度；
    
-   `activation` ：激活函数，对应于 $f(AW+b)$ ，默认为无激活函数（ `a(x) = x` ）。常用的激活函数包括 `tf.nn.relu` 、 `tf.nn.tanh` 和 `tf.nn.sigmoid` ；
    
-   `use_bias` ：是否加入偏置向量 `bias` ，即 $f(AW+b)$ 。默认为 `True` ；
    
-   `kernel_initializer` 、 `bias_initializer` ：权重矩阵 `kernel` 和偏置向量 `bias` 两个变量的初始化器。默认为 `tf.glorot_uniform_initializer` 。设置为 `tf.zeros_initializer` 表示将两个变量均初始化为全 0；
    

该层包含权重矩阵 `kernel = [input_dim, units]` 和偏置向量 `bias = [units]` [^1]；

这里着重从数学矩阵运算和线性变换的角度描述了全连接层。基于神经元建模的描述可参考 [[神经元]] 。

[^1]: 你可能会注意到， `tf.matmul(input, kernel)` 的结果是一个形状为 `[batch_size, units]` 的二维矩阵，这个二维矩阵要如何与形状为 `[units]` 的一维偏置向量 `bias` 相加呢？事实上，这里是 TensorFlow 的 Broadcasting 机制在起作用，该加法运算相当于将二维矩阵的每一行加上了 `Bias` 。