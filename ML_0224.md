
# ML_0224
Date: 2022-02-24 10:26
Source: [ML_0224]()

## Clean Data
### Scaling feature values
**Scaling** means converting floating-point feature values from their natural range (for example, 100 to 900) into a standard range (for example, 0 to 1 or -1 to +1)[<sup>1</sup>](#refer-anchor-1).
Z score relates the number of standard deviations away from the mean. In other words:$$ scaledvalue=(value-mean)/stddev$$
### Handling extreme outliers
### Binning[^1]

### Scrubbing
### Know your data
-   Keep in mind what you think your data should look like.
-   Verify that the data meets these expectations (or that you can explain why it doesnâ€™t).
-   Double-check that the training data agrees with other sources (for example, dashboards).

## Feature Crosses:Encoding Nonlinearity
A **feature cross** is a synthetic feature that encodes nonlinearity in the feature space by multiplying two or more input features together. (The term _cross_ comes from [_cross product_](https://wikipedia.org/wiki/Cross_product).) Let's create a feature cross named $x_3$ by crossing $x_1$ and $x_2$:

$$x_3=x_1x_2$$

We treat this newly minted x3 feature cross just like any other feature. The linear formula becomes:

$$y=b+w_1x_1+w_2x_2+w_3x_3$$




[^1]: Converting a (usually [**continuous**](https://developers.google.com/machine-learning/glossary#continuous_feature)) feature into multiple binary features called buckets or bins, typically based on value range. For example, instead of representing temperature as a single continuous floating-point feature, you could chop ranges of temperatures into discrete bins. Given temperature data sensitive to a tenth of a degree, all temperatures between 0.0 and 15.0 degrees could be put into one bin, 15.1 to 30.0 degrees could be a second bin, and 30.1 to 50.0 degrees could be a third bin.

